<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2020-09-24 Thu 21:53 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>paper-reading | vernacular.ai</title>
<meta name="generator" content="Org mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" href="./css/tufte.css" type="text/css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1>Paper Reading</h1>
<p class="subtitle">@ Vernacular.ai</p>

<style>
  #content {
    margin-top: 150px;
  }
  .subtitle {
    text-align: left;
    font-weight: normal;
  }
  .outline-text-1, .outline-text-2, .outline-text-3 {
    width: 55%;
    line-height: 2rem;
    margin-top: 1.4rem;
    margin-bottom: 1.4rem;
    padding-right: 0;
  }
  .badge {
    width: revert;
  }
  h2 {
    margin-top: 80px;
  }
  h3 {
    margin-top: 60px;
  }
  .outline-text-3 p {
    width: revert;
  }
  @media screen and (max-width: 760px) {
    .outline-text-1, .outline-text-2, .outline-text-3 {
      width: 100%;
    }
  }
</style>

<a href="./atom.xml">
  <img class="badge" alt="Atom url" src="https://img.shields.io/badge/follow-atom-blue?style=flat-square">
</a>
<a href="https://github.com/Vernacular-ai/paper-reading">
  <img class="badge" alt="GitHub last commit" src="https://img.shields.io/github/last-commit/vernacular-ai/paper-reading?style=flat-square">
</a>
<a href="https://github.com/Vernacular-ai/paper-reading/watchers">
  <img class="badge" alt="GitHub watchers" src="https://img.shields.io/github/watchers/Vernacular-ai/paper-reading?label=watch%20on%20github&style=flat-square">
</a>

<p>
List of papers we cover during our weekly paper reading session. For past and
missing links/notes, check out the (private) wiki.
</p>

<div id="outline-container-orgf502d65" class="outline-2">
<h2 id="orgf502d65">28 August, 2020</h2>
<div class="outline-text-2" id="text-orgf502d65">
</div>
<div id="outline-container-org12c8158" class="outline-3">
<h3 id="tang2010overlapping"><a id="org12c8158"></a>Overlapping experiment infrastructure: More, better, faster experimentation</h3>
<div class="outline-text-3" id="text-tang2010overlapping">
<pre class="example">
CUSTOM_ID: tang2010overlapping
YEAR: 2010
AUTHOR: Tang, Diane and Agarwal, Ashish and O'Brien, Deirdre and Meyer, Mike
</pre>
</div>
</div>

<div id="outline-container-org2ac9b68" class="outline-3">
<h3 id="schmit2019optimal"><a id="org2ac9b68"></a>Optimal testing in the experiment-rich regime</h3>
<div class="outline-text-3" id="text-schmit2019optimal">
<pre class="example">
CUSTOM_ID: schmit2019optimal
YEAR: 2019
AUTHOR: Schmit, Sven and Shah, Virag and Johari, Ramesh
</pre>
</div>
</div>
</div>

<div id="outline-container-orgbe82db4" class="outline-2">
<h2 id="orgbe82db4">21 August, 2020</h2>
<div class="outline-text-2" id="text-orgbe82db4">
</div>
<div id="outline-container-org8aae987" class="outline-3">
<h3 id="hou2020bridging"><a id="org8aae987"></a>Bridging Anaphora Resolution as Question Answering</h3>
<div class="outline-text-3" id="text-hou2020bridging">
<pre class="example">
CUSTOM_ID: hou2020bridging
YEAR: 2020
AUTHOR: Hou, Yufang
</pre>
</div>
</div>

<div id="outline-container-org4f56b69" class="outline-3">
<h3 id="suresh2019framework"><a id="org4f56b69"></a>A framework for understanding unintended consequences of machine learning</h3>
<div class="outline-text-3" id="text-suresh2019framework">
<pre class="example">
CUSTOM_ID: suresh2019framework
YEAR: 2019
AUTHOR: Suresh, Harini and Guttag, John V
</pre>
</div>
</div>
</div>

<div id="outline-container-orge378362" class="outline-2">
<h2 id="orge378362">14 August, 2020</h2>
<div class="outline-text-2" id="text-orge378362">
</div>
<div id="outline-container-orga24f7c2" class="outline-3">
<h3 id="kitaev2020reformer"><a id="orga24f7c2"></a>Reformer: The Efficient Transformer</h3>
<div class="outline-text-3" id="text-kitaev2020reformer">
<pre class="example">
CUSTOM_ID: kitaev2020reformer
YEAR: 2020
AUTHOR: Nikita Kitaev and Łukasz Kaiser and Anselm Levskaya
</pre>
</div>
</div>
</div>

<div id="outline-container-orgca52893" class="outline-2">
<h2 id="orgca52893">24 July, 2020</h2>
<div class="outline-text-2" id="text-orgca52893">
</div>
<div id="outline-container-org6d43fcb" class="outline-3">
<h3 id="elsayed2018adversarial"><a id="org6d43fcb"></a>Adversarial examples that fool both computer vision and time-limited humans</h3>
<div class="outline-text-3" id="text-elsayed2018adversarial">
<pre class="example">
CUSTOM_ID: elsayed2018adversarial
YEAR: 2018
AUTHOR: Elsayed, Gamaleldin and Shankar, Shreya and Cheung, Brian and Papernot, Nicolas and Kurakin, Alexey and Goodfellow, Ian and Sohl-Dickstein, Jascha
</pre>
</div>
</div>

<div id="outline-container-orge987563" class="outline-3">
<h3 id="brown2020language"><a id="orge987563"></a>Language models are few-shot learners</h3>
<div class="outline-text-3" id="text-brown2020language">
<pre class="example">
CUSTOM_ID: brown2020language
YEAR: 2020
AUTHOR: Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others
</pre>
</div>
</div>
</div>

<div id="outline-container-org85193ac" class="outline-2">
<h2 id="org85193ac">10 July, 2020</h2>
<div class="outline-text-2" id="text-org85193ac">
</div>
<div id="outline-container-org253b54e" class="outline-3">
<h3 id="bunk2020diet"><a id="org253b54e"></a>DIET: Lightweight Language Understanding for Dialogue Systems</h3>
<div class="outline-text-3" id="text-bunk2020diet">
<pre class="example">
CUSTOM_ID: bunk2020diet
YEAR: 2020
AUTHOR: Bunk, Tanja and Varshneya, Daksh and Vlasov, Vladimir and Nichol, Alan
</pre>
</div>
</div>

<div id="outline-container-org1c95b10" class="outline-3">
<h3 id="majumder2019dialoguernn"><a id="org1c95b10"></a>DialogueRNN: An attentive RNN for emotion detection in conversations</h3>
<div class="outline-text-3" id="text-majumder2019dialoguernn">
<pre class="example">
CUSTOM_ID: majumder2019dialoguernn
YEAR: 2019
AUTHOR: Majumder, Navonil and Poria, Soujanya and Hazarika, Devamanyu and Mihalcea, Rada and Gelbukh, Alexander and Cambria, Erik
</pre>
</div>
</div>
</div>

<div id="outline-container-orgb490d5d" class="outline-2">
<h2 id="orgb490d5d">3 July, 2020</h2>
<div class="outline-text-2" id="text-orgb490d5d">
</div>
<div id="outline-container-org0e5e175" class="outline-3">
<h3 id="wu2018starspace"><a id="org0e5e175"></a>StarSpace: Embed All The Things!</h3>
<div class="outline-text-3" id="text-wu2018starspace">
<pre class="example">
CUSTOM_ID: wu2018starspace
YEAR: 2018
AUTHOR: Wu, Ledell Yu and Fisch, Adam and Chopra, Sumit and Adams, Keith and Bordes, Antoine and Weston, Jason
</pre>
</div>
</div>

<div id="outline-container-orgba5b6ac" class="outline-3">
<h3 id="huang2020learning"><a id="orgba5b6ac"></a>Learning Asr-Robust Contextualized Embeddings for Spoken Language Understanding</h3>
<div class="outline-text-3" id="text-huang2020learning">
<pre class="example">
CUSTOM_ID: huang2020learning
YEAR: 2020
AUTHOR: Huang, Chao-Wei and Chen, Yun-Nung
</pre>
</div>
</div>
</div>

<div id="outline-container-orgd21adcc" class="outline-2">
<h2 id="orgd21adcc">12 June, 2020</h2>
<div class="outline-text-2" id="text-orgd21adcc">
</div>
<div id="outline-container-orgced2955" class="outline-3">
<h3 id="reimers2019sentence"><a id="orgced2955"></a>Sentence-bert: Sentence embeddings using siamese bert-networks</h3>
<div class="outline-text-3" id="text-reimers2019sentence">
<pre class="example">
CUSTOM_ID: reimers2019sentence
YEAR: 2019
AUTHOR: Reimers, Nils and Gurevych, Iryna
</pre>
</div>
</div>

<div id="outline-container-orgc34f548" class="outline-3">
<h3 id="paszke2019pytorch"><a id="orgc34f548"></a>PyTorch: An imperative style, high-performance deep learning library</h3>
<div class="outline-text-3" id="text-paszke2019pytorch">
<pre class="example">
CUSTOM_ID: paszke2019pytorch
YEAR: 2019
AUTHOR: Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others
</pre>
</div>
</div>

<div id="outline-container-orga808342" class="outline-3">
<h3 id="ramanujan2020s"><a id="orga808342"></a>What's Hidden in a Randomly Weighted Neural Network?</h3>
<div class="outline-text-3" id="text-ramanujan2020s">
<pre class="example">
CUSTOM_ID: ramanujan2020s
YEAR: 2020
AUTHOR: Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad
</pre>
</div>
</div>

<div id="outline-container-org962b058" class="outline-3">
<h3 id="patra2019weakly"><a id="org962b058"></a>Weakly Supervised Attention Networks for Entity Recognition</h3>
<div class="outline-text-3" id="text-patra2019weakly">
<pre class="example">
CUSTOM_ID: patra2019weakly
YEAR: 2019
AUTHOR: Patra, Barun and Moniz, Joel Ruben Antony
</pre>
</div>
</div>

<div id="outline-container-org4ec1971" class="outline-3">
<h3 id="kou2020improving"><a id="org4ec1971"></a>Improving BERT with Self-Supervised Attention</h3>
<div class="outline-text-3" id="text-kou2020improving">
<pre class="example">
CUSTOM_ID: kou2020improving
YEAR: 2020
AUTHOR: Kou, Xiaoyu and Yang, Yaming and Wang, Yujing and Zhang, Ce and Chen, Yiren and Tong, Yunhai and Zhang, Yan and Bai, Jing
</pre>
</div>
</div>
</div>

<div id="outline-container-org4743c17" class="outline-2">
<h2 id="org4743c17">5 June, 2020</h2>
<div class="outline-text-2" id="text-org4743c17">
</div>
<div id="outline-container-orga4de003" class="outline-3">
<h3 id="yang2016hierarchical"><a id="orga4de003"></a>Hierarchical attention networks for document classification</h3>
<div class="outline-text-3" id="text-yang2016hierarchical">
<pre class="example">
CUSTOM_ID: yang2016hierarchical
YEAR: 2016
AUTHOR: Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard
</pre>
</div>
</div>

<div id="outline-container-orgde10050" class="outline-3">
<h3 id="hancock2018training"><a id="orgde10050"></a>Training classifiers with natural language explanations</h3>
<div class="outline-text-3" id="text-hancock2018training">
<pre class="example">
CUSTOM_ID: hancock2018training
YEAR: 2018
AUTHOR: Hancock, Braden and Bringmann, Martin and Varma, Paroma and Liang, Percy and Wang, Stephanie and R, Christopher
</pre>
</div>
</div>

<div id="outline-container-org8fc2409" class="outline-3">
<h3 id="carmel2014entity"><a id="org8fc2409"></a>ERD'14: entity recognition and disambiguation challenge</h3>
<div class="outline-text-3" id="text-carmel2014entity">
<pre class="example">
CUSTOM_ID: carmel2014entity
YEAR: 2014
AUTHOR: Carmel, David and Chang, Ming-Wei and Gabrilovich, Evgeniy and Hsu, Bo-June and Wang, Kuansan
</pre>
</div>
</div>

<div id="outline-container-orge311053" class="outline-3">
<h3 id="carlini2018adversarial"><a id="orge311053"></a>Audio adversarial examples: Targeted attacks on speech-to-text</h3>
<div class="outline-text-3" id="text-carlini2018adversarial">
<pre class="example">
CUSTOM_ID: carlini2018adversarial
YEAR: 2018
AUTHOR: Carlini, Nicholas and Wagner, David
</pre>
</div>
</div>
</div>

<div id="outline-container-org02ee4aa" class="outline-2">
<h2 id="org02ee4aa">29 May, 2020</h2>
<div class="outline-text-2" id="text-org02ee4aa">
</div>
<div id="outline-container-orgd44d5cd" class="outline-3">
<h3 id="dhingra2020differentiable"><a id="orgd44d5cd"></a>Differentiable Reasoning over a Virtual Knowledge Base</h3>
<div class="outline-text-3" id="text-dhingra2020differentiable">
<pre class="example">
CUSTOM_ID: dhingra2020differentiable
YEAR: 2020
AUTHOR: Dhingra, Bhuwan and Zaheer, Manzil and Balachandran, Vidhisha and Neubig, Graham and Salakhutdinov, Ruslan and Cohen, William W
</pre>
</div>
</div>

<div id="outline-container-org127c0a3" class="outline-3">
<h3 id="shen2018naturaltts"><a id="org127c0a3"></a>Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</h3>
<div class="outline-text-3" id="text-shen2018naturaltts">
<pre class="example">
CUSTOM_ID: shen2018naturaltts
YEAR: 2018
AUTHOR: Shen, Jonathan and Pang, Ruoming and Weiss, Ron J and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and others
</pre>
</div>
</div>
</div>

<div id="outline-container-orgfd977ed" class="outline-2">
<h2 id="orgfd977ed">15 May, 2020</h2>
<div class="outline-text-2" id="text-orgfd977ed">
</div>
<div id="outline-container-org91f3375" class="outline-3">
<h3 id="wan2020nbdt"><a id="org91f3375"></a>NBDT: Neural-Backed Decision Trees</h3>
<div class="outline-text-3" id="text-wan2020nbdt">
<pre class="example">
CUSTOM_ID: wan2020nbdt
YEAR: 2020
AUTHOR: Wan, Alvin and Dunlap, Lisa and Ho, Daniel
</pre>
</div>
</div>

<div id="outline-container-org7d80ac7" class="outline-3">
<h3 id="choi2019echoing"><a id="org7d80ac7"></a>Faster Neural Network Training with Data Echoing</h3>
<div class="outline-text-3" id="text-choi2019echoing">
<pre class="example">
CUSTOM_ID: choi2019echoing
YEAR: 2019
AUTHOR: Choi, Dami and Passos, Alexandre and Shallue, Christopher J. and Dahl, George E.
</pre>
</div>
</div>

<div id="outline-container-org9e0fba0" class="outline-3">
<h3 id="howard2018universal"><a id="org9e0fba0"></a>Universal Language Model Fine-tuning for Text Classification</h3>
<div class="outline-text-3" id="text-howard2018universal">
<pre class="example">
CUSTOM_ID: howard2018universal
YEAR: 2018
AUTHOR: Howard, Jeremy and Ruder, Sebastian
</pre>
</div>
</div>
</div>

<div id="outline-container-org4579f0d" class="outline-2">
<h2 id="org4579f0d">8 May, 2020</h2>
<div class="outline-text-2" id="text-org4579f0d">
</div>
<div id="outline-container-org0c21d3a" class="outline-3">
<h3 id="bakshy2014designing"><a id="org0c21d3a"></a>Designing and Deploying Online Field Experiments</h3>
<div class="outline-text-3" id="text-bakshy2014designing">
<pre class="example">
CUSTOM_ID: bakshy2014designing
YEAR: 2014
AUTHOR: Bakshy, Eytan and Eckles, Dean and Bernstein, Michael S.
</pre>
</div>
</div>

<div id="outline-container-orgc5987b3" class="outline-3">
<h3 id="moore2010intelligent"><a id="orgc5987b3"></a>Intelligent Selection of Language Model Training Data</h3>
<div class="outline-text-3" id="text-moore2010intelligent">
<pre class="example">
CUSTOM_ID: moore2010intelligent
YEAR: 2010
AUTHOR: Moore, Robert C. and Lewis, William
</pre>
</div>
</div>

<div id="outline-container-org711ce09" class="outline-3">
<h3 id="rudin2019stop"><a id="org711ce09"></a>Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead</h3>
<div class="outline-text-3" id="text-rudin2019stop">
<pre class="example">
CUSTOM_ID: rudin2019stop
YEAR: 2019
AUTHOR: Rudin, Cynthia
</pre>
</div>
</div>
</div>

<div id="outline-container-orgb8a8a1d" class="outline-2">
<h2 id="orgb8a8a1d">25 April, 2020</h2>
<div class="outline-text-2" id="text-orgb8a8a1d">
</div>
<div id="outline-container-orgf3ab0d2" class="outline-3">
<h3 id="andrew2019gmail"><a id="orgf3ab0d2"></a>Gmail Smart Compose: Real-Time Assisted Writing</h3>
<div class="outline-text-3" id="text-andrew2019gmail">
<pre class="example">
CUSTOM_ID: andrew2019gmail
YEAR: 2019
AUTHOR: Andrew Dai and Benjamin Lee and Gagan Bansal and Jackie Tsay and Justin Lu and Mia Chen and Shuyuan Zhang and Tim Sohn and Yinan Wang and Yonghui Wu and Yuan Cao and Zhifeng Chen
</pre>
</div>
</div>

<div id="outline-container-org15ee59a" class="outline-3">
<h3 id="stoudenmire2017supervised"><a id="org15ee59a"></a>Supervised Learning with Quantum-Inspired Tensor Networks</h3>
<div class="outline-text-3" id="text-stoudenmire2017supervised">
<pre class="example">
CUSTOM_ID: stoudenmire2017supervised
YEAR: 2017
AUTHOR: Stoudenmire, Miles and Schwab, David
</pre>
</div>
</div>

<div id="outline-container-org9aff2e7" class="outline-3">
<h3 id="corbett2020measure"><a id="org9aff2e7"></a>The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning</h3>
<div class="outline-text-3" id="text-corbett2020measure">
<pre class="example">
CUSTOM_ID: corbett2020measure
YEAR: 2018
AUTHOR: Corbett-Davies, Sam and Goel, Sharad
</pre>
</div>
</div>

<div id="outline-container-orgee1ed25" class="outline-3">
<h3 id="zhang2017understanding"><a id="orgee1ed25"></a>Understanding deep learning requires rethinking generalization</h3>
<div class="outline-text-3" id="text-zhang2017understanding">
<pre class="example">
CUSTOM_ID: zhang2017understanding
YEAR: 2017
AUTHOR: Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol
</pre>
</div>
</div>
</div>

<div id="outline-container-orgc4924aa" class="outline-2">
<h2 id="orgc4924aa">17 April, 2020</h2>
<div class="outline-text-2" id="text-orgc4924aa">
</div>
<div id="outline-container-org8e75bf3" class="outline-3">
<h3 id="olah2020zoom"><a id="org8e75bf3"></a>Zoom In: An Introduction to Circuits</h3>
<div class="outline-text-3" id="text-olah2020zoom">
<pre class="example">
CUSTOM_ID: olah2020zoom
YEAR: 2020
AUTHOR: Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan
</pre>
</div>
</div>
</div>

<div id="outline-container-orgcce73d3" class="outline-2">
<h2 id="orgcce73d3">3 April, 2020</h2>
<div class="outline-text-2" id="text-orgcce73d3">
</div>
<div id="outline-container-orgbbf9494" class="outline-3">
<h3 id="malinowski2020sideways"><a id="orgbbf9494"></a>Sideways: Depth-Parallel Training of Video Models</h3>
<div class="outline-text-3" id="text-malinowski2020sideways">
<pre class="example">
CUSTOM_ID: malinowski2020sideways
YEAR: 2020
AUTHOR: Malinowski, Mateusz and Swirszcz, Grzegorz and Carreira, Joao and Patraucean, Viorica
</pre>
</div>
</div>

<div id="outline-container-org2a73fc9" class="outline-3">
<h3 id="oh2019speech2face"><a id="org2a73fc9"></a>Speech2face: Learning the face behind a voice</h3>
<div class="outline-text-3" id="text-oh2019speech2face">
<pre class="example">
CUSTOM_ID: oh2019speech2face
YEAR: 2019
AUTHOR: Oh, Tae-Hyun and Dekel, Tali and Kim, Changil and Mosseri, Inbar and Freeman, William T and Rubinstein, Michael and Matusik, Wojciech
</pre>
</div>
</div>

<div id="outline-container-orgfc27fbf" class="outline-3">
<h3 id="peters2011dialog"><a id="orgfc27fbf"></a>Dialog Methods for Improved Alphanumeric String Capture</h3>
<div class="outline-text-3" id="text-peters2011dialog">
<pre class="example">
CUSTOM_ID: peters2011dialog
YEAR: 2011
AUTHOR: Peters, Doug and Stubley, Peter
</pre>

<p>
Presents a way for dialog level collection of alpha numeric strings via an ASR.
Two main ideas:
</p>

<ol class="org-ol">
<li>Skip listing over n-best hypothesis across turns (attempts)</li>
<li>Chunking and confirming pieces one by one</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-orged19fba" class="outline-2">
<h2 id="orged19fba">28 February, 2020</h2>
<div class="outline-text-2" id="text-orged19fba">
</div>
<div id="outline-container-orgbedc2de" class="outline-3">
<h3 id="wu2019self"><a id="orgbedc2de"></a>Self-supervised dialogue learning</h3>
<div class="outline-text-3" id="text-wu2019self">
<pre class="example">
CUSTOM_ID: wu2019self
YEAR: 2019
AUTHOR: Wu, Jiawei and Wang, Xin and Wang, William Yang
</pre>

<p>
The self-supervision signal here is coming from a model which tries to predict
whether a provided tuple of turns is in order or not. Connecting this as the
discriminator in generative-discriminative dialog systems they find better
results.
</p>
</div>
</div>
</div>

<div id="outline-container-org5543d96" class="outline-2">
<h2 id="org5543d96">7 February, 2020</h2>
<div class="outline-text-2" id="text-org5543d96">
</div>
<div id="outline-container-org33a1193" class="outline-3">
<h3 id="hancock2019learning"><a id="org33a1193"></a>Learning from Dialogue after Deployment: Feed Yourself, Chatbot!</h3>
<div class="outline-text-3" id="text-hancock2019learning">
<pre class="example">
CUSTOM_ID: hancock2019learning
YEAR: 2019
AUTHOR: Hancock, Braden and Bordes, Antoine and Mazare, Pierre-Emmanuel and Weston, Jason
</pre>

<p>
This is an approach to collect supervision signal from deployment data. There
are three tasks for the system (which is a chat bot doing ranking on candidate
responses):
</p>

<ol class="org-ol">
<li>Dialogue. The main task. Given the turns till now, the bot ranks which
response to utter.</li>
<li>Satisfaction. Given turns till now, last being user utterance, predict
whether the user is satisfied.</li>
<li>Feedback. After asking for feedback from the user, predict user's response
(feedback) based on the turns till now.</li>
</ol>

<p>
The models have shared weights, mostly among task 1 and 3.
</p>
</div>
</div>
</div>

<div id="outline-container-orgcd536ab" class="outline-2">
<h2 id="orgcd536ab">31 January, 2020</h2>
<div class="outline-text-2" id="text-orgcd536ab">
</div>
<div id="outline-container-orgea4d9aa" class="outline-3">
<h3 id="bradley2019modeling"><a id="orgea4d9aa"></a>Modeling Sequences with Quantum States: A Look Under the Hood</h3>
<div class="outline-text-3" id="text-bradley2019modeling">
<pre class="example">
CUSTOM_ID: bradley2019modeling
YEAR: 2019
AUTHOR: Bradley, Tai-Danae and Stoudenmire, E Miles and Terilla, John
</pre>

<p>
This paper explores a new direction in language modelling. The idea is still to
learn the underlying distribution of sequences of characters, but here they do
it by learning the quantum analogue of the classical probability distribution
function. Unlike the classical case, marginal distributions there carry enough
information to re-construct the joint distribution. This is the central idea of
the paper, and is explained in the first half. The second half of the paper
explains the theory and implementation of the training algorithm, with a simple
example. Future work would be to apply this algorithm to a more complicated
example, and even adapt it to variable length sequences.
</p>
</div>
</div>

<div id="outline-container-orgf2d662f" class="outline-3">
<h3 id="gibiansky2017deep"><a id="orgf2d662f"></a>Deep voice 2: Multi-speaker neural text-to-speech</h3>
<div class="outline-text-3" id="text-gibiansky2017deep">
<pre class="example">
CUSTOM_ID: gibiansky2017deep
YEAR: 2017
AUTHOR: Gibiansky, Andrew and Arik, Sercan and Diamos, Gregory and Miller, John and Peng, Kainan and Ping, Wei and Raiman, Jonathan and Zhou, Yanqi
</pre>

<p>
This paper suggests improvements to <a href="https://arxiv.org/abs/1702.07825">DeepVoice</a> and <a href="https://arxiv.org/abs/1703.10135">Tacotron</a>, and also proposes a
way to add trainable speaker embeddings. The speaker embeddings are initialized
randomly and trained jointly through backpropagation. The paper lists some
patterns that lead to better performance
</p>

<ol class="org-ol">
<li>Transforming speaker embeddings to appropriate dimension and form for every
place it is added to the model. The transformed speaker embeddings are called
site-specific speaker embeddings</li>
<li>Initializing recurrent layer hidden states with the site-specific speaker
embeddings.</li>
<li>Concatenating the site-specific speaker embedding to input at every timestep
of the recurrent layer</li>
<li>Multiplying layer activations element-wise to the site-specific speaker
embeddings</li>
</ol>
</div>
</div>

<div id="outline-container-orge4b9e08" class="outline-3">
<h3 id="chang2016credit"><a id="orge4b9e08"></a>A credit assignment compiler for joint prediction</h3>
<div class="outline-text-3" id="text-chang2016credit">
<pre class="example">
CUSTOM_ID: chang2016credit
YEAR: 2016
AUTHOR: Chang, Kai-Wei and He, He and Ross, Stephane and Daume III, Hal and Langford, John
</pre>

<p>
This talks about an API for framing L2S style search problems in style of an
imperative program which allows for two optimizations:
</p>

<ol class="org-ol">
<li>memoization</li>
<li>forced path collapse, getting losses without going to the last state</li>
</ol>

<p>
Main reduction that happens here is to a cost-sensitive classification problem.
</p>
</div>
</div>
</div>

<div id="outline-container-org070619f" class="outline-2">
<h2 id="org070619f">17 January, 2020</h2>
<div class="outline-text-2" id="text-org070619f">
</div>
<div id="outline-container-orge32400d" class="outline-3">
<h3 id="vepstas2014learning"><a id="orge32400d"></a>Learning language from a large (unannotated) corpus</h3>
<div class="outline-text-3" id="text-vepstas2014learning">
<pre class="example">
CUSTOM_ID: vepstas2014learning
YEAR: 2014
AUTHOR: Vepstas, Linas and Goertzel, Ben
</pre>

<p>
Introductory paper on the general approach used in <a href="https://github.com/opencog/learn">learn</a>. The idea is to learn
various generalizable syntactic and semantic relations from unannotated corpus.
The relations are expressed using graphs sitting on top of link grammar and
meaning text theory (MTT). While the general approach is sketched out decently
enough, there are details to filled in various steps and experiments to run (as
of the writing in 2014).
</p>

<p>
On another note, the document is a nice read because of the many interesting
ways of looking at various ideas in understanding languages and going from
syntax to reasoning via semantics.
</p>
</div>
</div>
</div>

<div id="outline-container-org6b5987d" class="outline-2">
<h2 id="org6b5987d">10 January, 2020</h2>
<div class="outline-text-2" id="text-org6b5987d">
</div>
<div id="outline-container-orgef57ca1" class="outline-3">
<h3 id="sleator1995parsing"><a id="orgef57ca1"></a>Parsing English with a link grammar</h3>
<div class="outline-text-3" id="text-sleator1995parsing">
<pre class="example">
CUSTOM_ID: sleator1995parsing
YEAR: 1995
AUTHOR: Sleator, Daniel DK and Temperley, Davy
</pre>

<p>
We came to here via opencog's <a href="https://github.com/opencog/learn">learn</a> project. This is a nice perspective setup
also if you are missing out on formal introduction of grammars and all. Overall
a link grammar defines connectors on left and right side of a word with
disjunctions and conjunctions incorporated which then <i>link</i> together to form a
sentence, under certain constraints.
</p>

<p>
This specific paper shows the formulation and creates a parser for English,
covering many (not all) linguistics phenomena.
</p>
</div>
</div>
</div>

<div id="outline-container-org25aff94" class="outline-2">
<h2 id="org25aff94">20 December, 2019</h2>
<div class="outline-text-2" id="text-org25aff94">
</div>
<div id="outline-container-org4eb7e4c" class="outline-3">
<h3 id="wan2018generalized"><a id="org4eb7e4c"></a>Generalized end-to-end loss for speaker verification</h3>
<div class="outline-text-3" id="text-wan2018generalized">
<pre class="example">
CUSTOM_ID: wan2018generalized
YEAR: 2018
AUTHOR: Wan, Li and Wang, Quan and Papir, Alan and Moreno, Ignacio Lopez
</pre>

<p>
This paper is development over their previous research work, Tuple-based end to
end(TE2E) loss, for speaker identification. They try to generalize the concept
of the cosine similarity being used in TE2E by creating similarity matrics for
utterances by a user. They have suggested two losses in the paper:
</p>

<ol class="org-ol">
<li>Softmax loss</li>
<li>Contrast loss</li>
</ol>

<p>
Both these loss functions had two components, one which brings utterances by a
user together and others, which separates the utterances of different users. Out
of the two, Contrast loss is more rigorous.
</p>
</div>
</div>
</div>

<div id="outline-container-orged94d88" class="outline-2">
<h2 id="orged94d88">13 December, 2019</h2>
<div class="outline-text-2" id="text-orged94d88">
</div>
<div id="outline-container-org8b6e091" class="outline-3">
<h3 id="serdyuk2018towards"><a id="org8b6e091"></a>Towards end-to-end spoken language understanding</h3>
<div class="outline-text-3" id="text-serdyuk2018towards">
<pre class="example">
CUSTOM_ID: serdyuk2018towards
YEAR: 2018
AUTHOR: Serdyuk, Dmitriy and Wang, Yongqiang and Fuegen, Christian and Kumar, Anuj and Liu, Baiyang and Bengio, Yoshua
</pre>

<p>
This paper talks about developing an end to end model for intent recognition
form speech. Currently, all the models have several components like ASR and NLU,
which each have some errors of their own degrading the quality of the speech to
intent pipeline. Experiments for two tasks, speech to domain and speech to
intent were performed using the model. The model's architecture is mostly
inspired from end to end speech synthesis models. A unique feature of the
architecture is that they perform sub-sampling after the first GRU layer to
reduce the size of the vector and to tackle the problem of vanishing gradient.
</p>
</div>
</div>

<div id="outline-container-org687f66d" class="outline-3">
<h3 id="grathwohl2019classifier"><a id="org687f66d"></a>Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One</h3>
<div class="outline-text-3" id="text-grathwohl2019classifier">
<pre class="example">
CUSTOM_ID: grathwohl2019classifier
YEAR: 2019
AUTHOR: Will Grathwohl and Kuan-Chieh Wang and Jörn-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky
</pre>

<p>
They take a regular classifier, pick out logits before softmax and try to
formulate an energy based model able to give \(P(x, y)\) and \(P(x)\). The
formulation itself is pretty simple with the energy function being \(E(x) =
−LogSumExp_yf_\Theta(x)[y]\). Final loss sums cross entropy (for discriminative part)
and negative log likelhood of \(P(x)\) approximated using SGLD. Check out the repo
<a href="https://github.com/wgrathwohl/JEM">here</a>.
</p>

<p>
Although the learning mechanism is a little fragile and needs work to be
generally stable, the results are neat.
</p>
</div>
</div>
</div>

<div id="outline-container-org578864a" class="outline-2">
<h2 id="org578864a">29 November, 2019</h2>
<div class="outline-text-2" id="text-org578864a">
</div>
<div id="outline-container-orgc07b03d" class="outline-3">
<h3 id="re2019overton"><a id="orgc07b03d"></a>Overton: A Data System for Monitoring and Improving Machine-Learned Products</h3>
<div class="outline-text-3" id="text-re2019overton">
<pre class="example">
CUSTOM_ID: re2019overton
YEAR: 2019
AUTHOR: Ré, Christopher and Niu, Feng and Gudipati, Pallavi and Srisuwananukorn, Charles
</pre>

<p>
This is more about managing supervision than model. There are 3 problems that
they are trying to solve:
</p>
<ol class="org-ol">
<li>Fine grained quality monitoring,</li>
<li>Support for multi-component pipelines, and</li>
<li>Updating supervision</li>
</ol>

<p>
For this, they make easy to use abstractions for describing supervision and
developing models. They also do a lot of multitask learning and snorkelish weak
supervision, including the recent slicing abstractions for fine grained quality
control.
</p>

<p>
While you have to adapt a few pieces for your own case (and scale), Overton is a
nice testimony for success of things like weak supervision and higher level
development abstractions in production.
</p>
</div>
</div>

<div id="outline-container-org1a3982d" class="outline-3">
<h3 id="chen2019slice"><a id="org1a3982d"></a>Slice-based learning: A programming model for residual learning in critical data slices</h3>
<div class="outline-text-3" id="text-chen2019slice">
<pre class="example">
CUSTOM_ID: chen2019slice
YEAR: 2019
AUTHOR: Chen, Vincent and Wu, Sen and Ratner, Alexander J and Weng, Jen and Ré, Christopher
</pre>

<p>
This is taking the snorkel's labelling function idea to group data instances in
<i>slices</i>, segments which are interesting to us from an overall quality
perspective. These slicing functions are important not only for identifying and
narrowing down to specific kinds of data instances but also for learning slice
specific representations which works out as computationally cheap way (there are
other benefits too) of replicating a Mixture of Experts style model.
</p>

<p>
Like with labelling functions, we have the slice membership predicted using
heuristics which are noisy. This membership value along with slice
representations (and slice prediction confidences) help create the slice aware
representation to be used for the final task. The appendix has few good examples
of slicing functions.
</p>
</div>
</div>
</div>

<div id="outline-container-org29a85ec" class="outline-2">
<h2 id="org29a85ec">21 September, 2019</h2>
<div class="outline-text-2" id="text-org29a85ec">
<ul class="org-ul">
<li>Moody, C. E., <a href="https://arxiv.org/abs/1605.02019">Mixing dirichlet topic models and word embeddings to make lda2vec</a>, arXiv preprint arXiv:1605.02019, (),  (2016). (cite:moody2016mixing)</li>

<li>Ren, L., Xie, K., Chen, L., &amp; Yu, K., <a href="https://arxiv.org/pdf/1810.09587.pdf">Towards universal dialogue state tracking</a>, arXiv preprint arXiv:1810.09587, (),  (2018). (cite:ren2018towards)</li>

<li>Coucke, A., Saade, A., Ball, A., Th\'eodore Bluche, Caulier, A., Leroy, D., Cl\'ement Doumouro, …, <a href="http://arxiv.org/abs/1805.10190">Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces</a>, CoRR, abs/1805.10190(),  (2018). (cite:DBLP:journals/corr/abs-1805-10190)</li>
</ul>
</div>
</div>

<div id="outline-container-orgbf5e0e4" class="outline-2">
<h2 id="orgbf5e0e4">3 August, 2019</h2>
<div class="outline-text-2" id="text-orgbf5e0e4">
<ul class="org-ul">
<li>Kim, S., Eriksson, T., Kang, H., &amp; Hee Youn, D., <a href="https://www.researchgate.net/publication/4087401_A_pitch_synchronous_feature_extraction_method_for_speaker_recognition/link/00b7d5364b1a66dafc000000/download">A pitch synchronous feature extraction method for speaker recognition</a>, In ,  (pp. ) (2004). : . (cite:PSMFCC)</li>

<li>Chen, J., <a href="http://www.columbia.edu/~jcc2161/documents/HumanVoice.pdf">Elements of human voice</a> (2016), : . (cite:HumanVoice)</li>

<li>Ghorbani, A., &amp; Zou, J., <a href="http://proceedings.mlr.press/v97/ghorbani19c/ghorbani19c.pdf">Data shapley: equitable valuation of data for machine learning</a>, arXiv preprint arXiv:1904.02868, (),  (2019). (cite:ghorbani2019data)</li>

<li>Shen, G., Horikawa, T., Majima, K., &amp; Kamitani, Y., <a href="https://journals.plos.org/ploscompbiol/article?rev=1&amp;id=10.1371/journal.pcbi.1006633">Deep image reconstruction from human brain activity</a>, PLoS computational biology, 15(1), 1006633 (2019). (cite:shen2019deep)</li>

<li>Daum\'e III, Hal, <a href="http://legacydirs.umiacs.umd.edu/~hal/docs/daume07easyadapt.pdf">Frustratingly easy domain adaptation</a>, arXiv preprint arXiv:0907.1815, (),  (2009). (cite:daume2009frustratingly)</li>
</ul>
</div>
</div>

<div id="outline-container-org1206105" class="outline-2">
<h2 id="org1206105">27 July, 2019</h2>
<div class="outline-text-2" id="text-org1206105">
<ul class="org-ul">
<li>Belkin, M., Hsu, D., Ma, S., &amp; Mandal, S., <a href="https://arxiv.org/pdf/1812.11118.pdf">Reconciling modern machine learning and the bias-variance trade-off</a>, arXiv preprint arXiv:1812.11118, (),  (2018). (cite:belkin2018reconciling)</li>
</ul>
</div>
</div>

<div id="outline-container-org5873f82" class="outline-2">
<h2 id="org5873f82">20 July, 2019</h2>
<div class="outline-text-2" id="text-org5873f82">
<ul class="org-ul">
<li>Locatello, F., Bauer, S., Lucic, M., Gelly, S., Sch\"olkopf, Bernhard, &amp; Bachem, O., <a href="https://arxiv.org/pdf/1811.12359.pdf">Challenging common assumptions in the unsupervised learning of disentangled representations</a>, arXiv preprint arXiv:1811.12359, (),  (2018). (cite:locatello2018challenging)</li>
</ul>
</div>
</div>

<div id="outline-container-orgf52cd60" class="outline-2">
<h2 id="orgf52cd60">13 July, 2019</h2>
<div class="outline-text-2" id="text-orgf52cd60">
<ul class="org-ul">
<li>Advani, M. S., &amp; Saxe, A. M., <a href="https://arxiv.org/abs/1710.03667">High-dimensional dynamics of generalization error in neural networks</a>, arXiv preprint arXiv:1710.03667, (),  (2017). (cite:advani2017high)</li>
</ul>
</div>
</div>

<div id="outline-container-org13bc6ea" class="outline-2">
<h2 id="org13bc6ea">6 July, 2019</h2>
<div class="outline-text-2" id="text-org13bc6ea">
<ul class="org-ul">
<li>Friedman, J., Hastie, T., &amp; Tibshirani, R., <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">The elements of statistical learning</a>, In  (Eds.),  (pp. 51–61) (2001). : Springer series in statistics New York. (cite:friedman2001elements)</li>

<li>Barham, P., &amp; Isard, M., <a href="https://dl.acm.org/citation.cfm?id=3321441">Machine learning systems are stuck in a rut</a>, In , Proceedings of the Workshop on Hot Topics in Operating Systems (pp. 177–183) (2019). New York, NY, USA: ACM. (cite:barham2019machine)</li>

<li>Hastie, T., Montanari, A., Rosset, S., &amp; Tibshirani, R. J., <a href="http://www.stat.cmu.edu/~ryantibs/papers/lsinter.pdf">Surprises in high-dimensional ridgeless least squares interpolation</a>, arXiv preprint arXiv:1903.08560, (),  (2019). (cite:hastie2019surprises)</li>

<li>Levitan, S. I., Mishra, T., &amp; Bangalore, S., <a href="http://www.cs.columbia.edu/~sarahita/papers/speech_prosody16.pdf">Automatic identification of gender from speech</a>, In , Proceeding of Speech Prosody (pp. 84–88) (2016). : . (cite:levitan2016automatic)</li>
</ul>
</div>
</div>

<div id="outline-container-org4786825" class="outline-2">
<h2 id="org4786825">1 July, 2019</h2>
<div class="outline-text-2" id="text-org4786825">
<ul class="org-ul">
<li>Friedman, J., Hastie, T., &amp; Tibshirani, R., <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">The elements of statistical learning</a>, In  (Eds.),  (pp. 51–61) (2001). : Springer series in statistics New York. (cite:friedman2001elements)</li>

<li>Graf, S., Herbig, T., Buck, M., &amp; Schmidt, G., <a href="https://asp-eurasipjournals.springeropen.com/track/pdf/10.1186/s13634-015-0277-z">Features for voice activity detection: a comparative analysis</a>, EURASIP Journal on Advances in Signal Processing, 2015(1), 91 (2015). (cite:graf2015features)</li>

<li>Welling, M., &amp; Teh, Y. W., <a href="https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf">Bayesian learning via stochastic gradient langevin dynamics</a>, In , Proceedings of the 28th international conference on machine learning (ICML-11) (pp. 681–688) (2011). : . (cite:welling2011bayesian)</li>

<li>Goodman, J., <a href="https://arxiv.org/pdf/cs/0108005.pdf">A bit of progress in language modeling</a>, arXiv preprint arXiv:cs/0108005, (),  (2001). (cite:goodman2001progress)</li>

<li>Cotterell, R., Mielke, S. J., Eisner, J., &amp; Roark, B., <a href="https://www.aclweb.org/anthology/N18-2085">Are all languages equally hard to language-model?</a>, In , Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) (pp. 536–541) (2018). New Orleans, Louisiana: Association for Computational Linguistics. (cite:cotterell-etal-2018-languages)</li>
</ul>
</div>
</div>

<div id="outline-container-orgabf38ec" class="outline-2">
<h2 id="orgabf38ec">25 June, 2019</h2>
<div class="outline-text-2" id="text-orgabf38ec">
<ul class="org-ul">
<li>Reynolds, D. A., Quatieri, T. F., &amp; Dunn, R. B., <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.338&amp;rep=rep1&amp;type=pdf">Speaker verification using adapted gaussian mixture models</a>, Digital signal processing, 10(1-3), 19–41 (2000). (cite:reynolds2000speaker)</li>

<li>Jasper Snoek, H. L., &amp; Adams, R. P., <a href="https://arxiv.org/pdf/1206.2944.pdf">Practical bayesian optimization of machine learning algorithms</a>, arXiv preprint arXiv:1206.2944, (),  (2012). (cite:snoek2012practical)</li>

<li>Breck, E., Zinkevich, M., Polyzotis, N., Whang, S., &amp; Roy, S., <a href="https://www.sysml.cc/doc/2019/167.pdf">Data validation for machine learning</a>, In , Proceedings of SysML (pp. ) (2019). : . (cite:breck2019data)</li>

<li>Carbonell, J. G., <a href="https://link.springer.com/chapter/10.1007/978-3-662-12405-5_5">Learning by analogy: formulating and generalizing plans from past experience</a>, In  (Eds.), Machine learning (pp. 137–161) (1983). : Springer. (cite:carbonell1983learning)</li>

<li>Liu, B., Wang, L., Liu, M., &amp; Xu, C., <a href="https://arxiv.org/abs/1901.06455">Lifelong federated reinforcement learning: a learning architecture for navigation in cloud robotic systems</a>, , abs/1901.06455(),  (2019). (cite:Liu2019LifelongFR)</li>
</ul>
</div>
</div>

<div id="outline-container-org3929649" class="outline-2">
<h2 id="org3929649">15 June, 2019</h2>
<div class="outline-text-2" id="text-org3929649">
<ul class="org-ul">
<li>Mohri, M., Pereira, F., &amp; Riley, M., <a href="http://www.sciencedirect.com/science/article/pii/S0885230801901846">Weighted finite-state transducers in speech recognition</a>, Computer Speech &amp; Language, 16(1), 69–88 (2002). (cite:MOHRI200269)</li>

<li>Ueffing, N., Bisani, M., &amp; Vozila, P., <a href="https://research.nuance.com/wp-content/uploads/2014/11/AutoPunc_Interspeech2013_paper_finalsubmission.pdf">Improved models for automatic punctuation prediction for spoken and written text.</a>, In , Interspeech (pp. 3097–3101) (2013). : . (cite:ueffing2013improved)</li>

<li>Liu, Z., Miao, Z., Zhan, X., Wang, J., Gong, B., &amp; Yu, S. X., <a href="https://arxiv.org/abs/1904.05160">Large-scale long-tailed recognition in an open world</a>, arXiv preprint arXiv:1904.05160, (),  (2019). (cite:liu2019large)</li>

<li>Iyer, A., Jonnalagedda, M., Parthasarathy, S., Radhakrishna, A., &amp; Rajamani, S. K., <a href="https://www.microsoft.com/en-us/research/publication/synthesis-and-machine-learning-for-heterogeneous-extraction/">Synthesis and machine learning for heterogeneous extraction</a>, In , Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation (pp. 301–315) (2019). : . (cite:iyer2019synthesis)</li>
</ul>
</div>
</div>

<div id="outline-container-org16f21c4" class="outline-2">
<h2 id="org16f21c4">8 June, 2019</h2>
<div class="outline-text-2" id="text-org16f21c4">
<ul class="org-ul">
<li>Dehak, N., Kenny, P. J., Dehak, R\'eda, Dumouchel, P., &amp; Ouellet, P., <a href="https://ieeexplore.ieee.org/document/5545402">Front-end factor analysis for speaker verification</a>, IEEE Transactions on Audio, Speech, and Language Processing, 19(4), 788–798 (2010). (cite:dehak2010front)</li>

<li>Dehak, N., Dehak, R., Kenny, P., Br\"ummer, Niko, Ouellet, P., &amp; Dumouchel, P., <a href="https://www.crim.ca/perso/patrick.kenny/IS090079.PDF">Support vector machines versus fast scoring in the low-dimensional total variability space for speaker verification</a>, In , Tenth Annual conference of the international speech communication association (pp. ) (2009). : . (cite:dehak2009support)</li>

<li>Sutton, C., &amp; McCallum, A., <a href="https://people.cs.umass.edu/~mccallum/papers/crf-tutorial.pdf">An introduction to conditional random fields for relational learning</a>, In  (Eds.), Introduction to Statistical Relational Learning (pp. ) (2006). : . (cite:sutton06introduction)</li>

<li>Mendis, C., Droppo, J., Maleki, S., Musuvathi, M., Mytkowicz, T., &amp; Zweig, G., <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/ParallelizingWFSTSpeechDecoders.ICASSP2016.pdf">Parallelizing wfst speech decoders</a>, In , 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 5325–5329) (2016). : . (cite:mendis2016parallelizing)</li>
</ul>
</div>
</div>

<div id="outline-container-orgde3e006" class="outline-2">
<h2 id="orgde3e006">1 June, 2019</h2>
<div class="outline-text-2" id="text-orgde3e006">
<ul class="org-ul">
<li>Russo, D. J., Van Roy, B., Kazerouni, A., Osband, I., Wen, Z., &amp; others, , <a href="https://arxiv.org/abs/1707.02038">A tutorial on thompson sampling</a>, Foundations and Trends{\textregistered} in Machine Learning, 11(1), 1–96 (2018). (cite:russo2018tutorial)</li>
</ul>
</div>
</div>

<div id="outline-container-org6ac6190" class="outline-2">
<h2 id="org6ac6190">18 May, 2019</h2>
<div class="outline-text-2" id="text-org6ac6190">
<ul class="org-ul">
<li>Gravano, A., Jansche, M., &amp; Bacchiani, M., <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/34562.pdf">Restoring punctuation and capitalization in transcribed speech</a>, In , 2009 IEEE International Conference on Acoustics, Speech and Signal Processing (pp. 4741–4744) (2009). : . (cite:gravano2009restoring)</li>

<li>Mintz, M., Bills, S., Snow, R., &amp; Jurafsky, D., <a href="https://web.stanford.edu/~jurafsky/mintz.pdf">Distant supervision for relation extraction without labeled data</a>, In , Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2 (pp. 1003–1011) (2009). : . (cite:mintz2009distant)</li>

<li>Beygelzimer, A., Daum\'e, Hal, Langford, J., &amp; Mineiro, P., <a href="https://arxiv.org/abs/1502.02704">Learning reductions that really work</a>, Proceedings of the IEEE, 104(1), 136–147 (2016). (cite:beygelzimer2016learning)</li>
</ul>
</div>
</div>

<div id="outline-container-orgf05f600" class="outline-2">
<h2 id="orgf05f600">13 May, 2019</h2>
<div class="outline-text-2" id="text-orgf05f600">
<ul class="org-ul">
<li>Sculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner, D., Chaudhary, V., …, <a href="https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems">Hidden technical debt in machine learning systems</a>, In , Advances in neural information processing systems (pp. 2503–2511) (2015). : . (cite:sculley2015hidden)</li>

<li>Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., …, <a href="https://arxiv.org/abs/1609.08144">Google's neural machine translation system: bridging the gap between human and machine translation</a>, arXiv preprint arXiv:1609.08144, (),  (2016). (cite:wu2016google)</li>

<li>Ghahramani, Z., <a href="https://www.inf.ed.ac.uk/teaching/courses/pmr/docs/ul.pdf">Unsupervised learning</a>, In , Summer School on Machine Learning (pp. 72–112) (2003). : . (cite:ghahramani2003unsupervised)</li>

<li>Hundman, K., Constantinou, V., Laporte, C., Colwell, I., &amp; Soderstrom, T., <a href="https://arxiv.org/abs/1802.04431">Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding</a>, In , Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining (pp. 387–395) (2018). : . (cite:hundman2018detecting)</li>
</ul>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-141179193-2"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-141179193-2');
</script>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2020-09-24 Thu 21:53</p>
<p class="creator"><a href="https://www.gnu.org/software/emacs/">Emacs</a> 26.3 (<a href="https://orgmode.org">Org</a> mode 9.1.9)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>